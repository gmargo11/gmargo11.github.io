---
layout: post
title:  "Hierarchical Deep Q-Learning (6.882 Final Project)"
date:   2019-05-30 18:19:00 -0400
categories: jekyll update
---

<img align="right" width="150" src="/img/montezumas-revenge.JPG" style="padding: 0 35px">
<img align="left" width="100" src="/img/profile-img.jpg" style="padding: 0 35px">


This spring, I took the class 6.882: Embodied Intelligence. The class was taught by Tomas Lozano-Perez, Leslie Kaelbling, and Phillip Isola, three faculty at MIT CSAIL. The coursework consisted of reading and discussing research papers on perception, learning, and planning for physical agents operating in the real world. The collection of papers selected delivered a history of artificial intelligence in robotics as well as a sampling of recent state-of-the-art works.

The course culminated with an open-ended final project. For my final project, I chose to replicate and analyze the results of a 2016 paper: "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation", by Tejas Kulkarni, Josh Tenenbaum, and others (available [here](https://arxiv.org/abs/1604.06057)). This paper applied deep neural networks to learn the Q-value functions in a hierarchically structured learning problem. This approach successfully achieved a nonzero score in the notoriously challenging Atari game Montezuma's Revenge, a game which is difficult for the original non-hierarchical deep Q-learning algorithm due to its sparse, delayed rewards.

I particularly enjoyed working with this paper for a few reasons. One reason was that although the paper dealt with deep reinforcement learning, it provided a demonstration of its results first in a very small and simple environment, which was valuable given my limited computational resources for this project. Another reason I enjoyed working with this paper was that its authors presented their work in the context of cognitive science. Another Tenenbaum paper we read for the class, "Building Machines That Learn and Think Like People", more thoroughly inspected the gap between artificial and human intelligence.

The final product of my project was an implementation of the hierarchical deep Q-learning algorithm (h-DQN), a report describing my successful replication of results, and a presentation delivered on our final day of class. You can check out all these materials below:

[Code](https://github.com/gmargo11/hDQN "Code") | [Report](https://github.com/gmargo11/hDQN/blob/master/paper.pdf "Report") | [Presentation](https://github.com/gmargo11/hDQN/blob/master/presentation.pdf "Presentation")