---
layout: post
title:  "Hierarchical Deep Q-Learning (6.882 Final Project)"
date:   2019-05-30 18:19:00 -0400
categories: jekyll update
---

This spring, I took the class 6.882: Embodied Intelligence. The class was taught by Tomas Lozano-Perez, Leslie Kaelbling, and Phillip Isola, three faculty at MIT CSAIL. The coursework consisted of reading and discussing research papers on perception, learning, and planning for physical agents operating in the real world. The collection of papers selected delivered a history of artificial intelligence in robotics as well as a sampling of recent state-of-the-art works.

The course culminated with an open-ended final project. For my final project, I chose to replicate and analyze the results of a 2016 paper: "Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation", by Tejas Kulkarni, Josh Tenenbaum, and others (available [here](https://arxiv.org/abs/1604.06057)). I enjoyed working with this paper for a few reasons. One reason was that although the paper deals with deep reinforcement learning, it provides a demonstration of its results first in a very small and simple environment, which was valuable given my limited computational resources for this project. Another reason I enjoyed working with this paper was that its authors present their work in the context of cognitive science. Another Tenenbaum paper we read for the class, "Building Machines That Learn and Think Like People", more thoroughly inspects the gap between artificial and human intelligence.

The final product of my project was an implementation of the hierarchical deep Q-learning algorithm (h-DQN), a report describing my successful replication of results, and a presentation delivered on our final day of class. You can check out all these materials below:

[Code](https://github.com/gmargo11/hDQN "Code") [Report](https://github.com/gmargo11/hDQN/blob/master/paper.pdf "Report") [Presentation](https://github.com/gmargo11/hDQN/blob/master/presentation.pdf "Presentation")